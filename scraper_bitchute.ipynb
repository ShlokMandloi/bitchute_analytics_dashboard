{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6352f7a7",
   "metadata": {},
   "source": [
    "## BITCHUTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee647c4a-ce6c-4717-a60b-60f780a666c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas==2.1.4\n",
      "time==Standard library, no version\n",
      "markdownify==0.12.1\n",
      "beautifulsoup4==4.12.2\n",
      "selenium==4.22.0\n",
      "webdriver_manager==4.0.1\n",
      "python-dateutil==2.8.2\n",
      "tqdm==4.65.0\n",
      "retrying==1.3.4\n",
      "requests==2.31.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import markdownify\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from retrying import retry\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from dateutil.parser import parse as dateutil_parse\n",
    "import requests\n",
    "import pkg_resources\n",
    "\n",
    "print(f\"pandas=={pd.__version__}\")\n",
    "print(\"time==Standard library, no version\")\n",
    "print(f\"markdownify=={pkg_resources.get_distribution('markdownify').version}\")\n",
    "print(f\"beautifulsoup4=={pkg_resources.get_distribution('beautifulsoup4').version}\")\n",
    "print(f\"selenium=={pkg_resources.get_distribution('selenium').version}\")\n",
    "print(f\"webdriver_manager=={pkg_resources.get_distribution('webdriver_manager').version}\")\n",
    "print(f\"python-dateutil=={pkg_resources.get_distribution('python-dateutil').version}\")\n",
    "print(f\"tqdm=={pkg_resources.get_distribution('tqdm').version}\")\n",
    "print(f\"retrying=={pkg_resources.get_distribution('retrying').version}\")\n",
    "print(f\"requests=={pkg_resources.get_distribution('requests').version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a337af25-025d-4562-91f6-aed7ddf143e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving: https://api.bitchute.com/category/news/ Retrieving: https://api.bitchute.com/category/news/ "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple Python module for retrieving data from bitchute.\n",
    "# Copyright (C) 2022 Marcus Burkhardt\n",
    "#\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
    "\n",
    "import time\n",
    "import markdownify\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from retrying import retry\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from dateutil.parser import parse as dateutil_parse\n",
    "import requests\n",
    "\n",
    "class Crawler():\n",
    "    def __init__(self, headless=True, verbose=False, chrome_driver=None):\n",
    "        self.options = Options()\n",
    "        if headless:\n",
    "            self.options.add_argument('--headless')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.chrome_driver = chrome_driver\n",
    "        self.wd = None\n",
    "        self.status = []\n",
    "        self.verbose = verbose\n",
    "        self.bitchute_base = 'https://api.bitchute.com/category/news/'\n",
    "        self.channel_base = 'https://api.bitchute.com/channel/{}/'\n",
    "        self.video_base = 'https://api.bitchute.com/video/'\n",
    "        self.hashtag_base = 'https://api.bitchute.com/hashtag/{}/'\n",
    "        self.profile_base = 'https://api.bitchute.com/profile/{}/'\n",
    "        self.search_base = 'https://api.bitchute.com/search/?query={}&kind=video'\n",
    "\n",
    "    def create_webdriver(self):\n",
    "        if not self.chrome_driver:\n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.wd = webdriver.Chrome(service=service, options=self.options)\n",
    "        else:\n",
    "            self.wd = webdriver.Chrome(self.chrome_driver, options=self.options)\n",
    "\n",
    "    def reset_webdriver(self):\n",
    "        if self.wd:\n",
    "            self.wd.quit()\n",
    "        self.wd = None\n",
    "        \n",
    "    def fetch(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "            return response.text\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to fetch URL {url}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def parse_video_ids(self, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        video_ids = []\n",
    "        if soup.find(class_='results-list'):\n",
    "            for result in soup.find_all(class_='video-result-container'):\n",
    "                if result.find(class_='video-result-title'):\n",
    "                    video_id = result.find(class_='video-result-title').find('a').get('href').split('/')[-2]\n",
    "                    video_ids.append(video_id)\n",
    "        return video_ids\n",
    "\n",
    "    @retry(stop_max_attempt_number=5, wait_random_min=1000, wait_random_max=2000)\n",
    "    def call(self, url, click_link_text=None, scroll=True, top=None):\n",
    "        if not self.wd:\n",
    "            self.create_webdriver()\n",
    "        if self.verbose:\n",
    "            print('Retrieving: ' + url + ' ', end='')\n",
    "        self.set_status('Retrieving: ' + url)\n",
    "\n",
    "        if self.wd.current_url == url:\n",
    "            self.wd.get('about:blank')\n",
    "            self.wd.get(url)\n",
    "        else:\n",
    "            self.wd.get(url)\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "        if len(self.wd.find_elements(By.XPATH, '//button[normalize-space()=\"Dismiss\"]')) > 0:\n",
    "            time.sleep(2)\n",
    "            self.wd.find_element(By.XPATH, '//button[normalize-space()=\"Dismiss\"]').click()\n",
    "\n",
    "        if click_link_text and not len(self.wd.find_elements(By.PARTIAL_LINK_TEXT, click_link_text)) > 0:\n",
    "            time.sleep(5)\n",
    "\n",
    "        if click_link_text:\n",
    "            if len(self.wd.find_elements(By.PARTIAL_LINK_TEXT, click_link_text)) > 0:\n",
    "                time.sleep(2)\n",
    "                self.wd.find_element(By.PARTIAL_LINK_TEXT, click_link_text).click()\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                print('Cannot find link to click')\n",
    "\n",
    "        sensitivity = 'Some videos are not shown'\n",
    "        if len(self.wd.find_elements(By.PARTIAL_LINK_TEXT, sensitivity)) > 0:\n",
    "            self.wd.find_element(By.PARTIAL_LINK_TEXT, sensitivity).click()\n",
    "            time.sleep(2)\n",
    "\n",
    "        if scroll:\n",
    "            if top:\n",
    "                iterations = (top // 10) + (top % 10 > 0)\n",
    "                iteration = 1\n",
    "                increment = 1\n",
    "            else:\n",
    "                iterations = 1\n",
    "                iteration = 0\n",
    "                increment = 0\n",
    "\n",
    "            script = (\n",
    "                'window.scrollTo(0, document.body.scrollHeight);'\n",
    "                'var lenOfPage=document.body.scrollHeight;'\n",
    "                'return lenOfPage;'\n",
    "            )\n",
    "\n",
    "            lenOfPage = self.wd.execute_script(script)\n",
    "            match = False\n",
    "            while not match and iteration < iterations:\n",
    "                iteration += increment\n",
    "                if self.verbose:\n",
    "                    print('.', end='')\n",
    "                self.set_status('.')\n",
    "                lastCount = lenOfPage\n",
    "                time.sleep(4)\n",
    "                lenOfPage = self.wd.execute_script(script)\n",
    "                if lastCount == lenOfPage:\n",
    "                    match = True\n",
    "        if self.verbose:\n",
    "            print('')\n",
    "\n",
    "        page_source = self.wd.page_source\n",
    "        return page_source\n",
    "\n",
    "    def process_views(self, views):\n",
    "        if \"k\" in views or \"K\" in views:\n",
    "            views = views.replace('K', '').replace('k', '')\n",
    "            if '.' not in views:\n",
    "                views = views[:-1] + '.' + views[-1:]\n",
    "            views = float(views) * 1000\n",
    "        elif \"m\" in views or \"M\" in views:\n",
    "            views = views.replace('M', '').replace('m', '')\n",
    "            if '.' in views:\n",
    "                views = float(views)\n",
    "            else:\n",
    "                views = float(views) / 10\n",
    "            views = views * 1000000\n",
    "        return int(views)\n",
    "\n",
    "    def search(self, query, top=100):\n",
    "        '''\n",
    "        Queries Bitchute and retrieves top n results according to the relevance ranking.\n",
    "\n",
    "        Parameters:\n",
    "        query (str): Search string\n",
    "        top (int): Number of results to be retrieved\n",
    "\n",
    "        Returns:\n",
    "        data: Dataframe of search results.\n",
    "        '''\n",
    "        url = self.search_base.format(query)\n",
    "        if isinstance(top, str) and top.lower() == 'all':\n",
    "            top = None\n",
    "        src = self.call(url, top=top)\n",
    "        data = self.parser(src, type='video_search')\n",
    "        self.reset_webdriver()\n",
    "        return data\n",
    "\n",
    "    def get_recommended_videos(self, type='popular'):\n",
    "        '''\n",
    "        Scapes recommended videos on bitchute homepage.\n",
    "\n",
    "        Parameters:\n",
    "        type (str): POPULAR, TRENDING, ALL\n",
    "\n",
    "        Returns:\n",
    "        data: Dataframe of recommended videos.\n",
    "        '''\n",
    "        if type == 'popular':\n",
    "            src = self.call(self.bitchute_base)\n",
    "            data = self.parser(src, type='recommended_videos', kind='popular')\n",
    "            return data\n",
    "        elif type == 'trending':\n",
    "            src = self.call(self.bitchute_base, click_link_text='TRENDING')\n",
    "            data = self.parser(src, type='recommended_videos', kind='trending-day')\n",
    "            return data\n",
    "        elif type == 'trending-day':\n",
    "            src = self.call(self.bitchute_base, click_link_text='TRENDING')\n",
    "            data = self.parser(src, type='recommended_videos', kind='trending-day')\n",
    "            return data\n",
    "        elif type == 'trending-week':\n",
    "            src = self.call(self.bitchute_base, click_link_text='TRENDING')\n",
    "            data = self.parser(src, type='recommended_videos', kind='trending-week')\n",
    "            return data\n",
    "        elif type == 'trending-month':\n",
    "            src = self.call(self.bitchute_base, click_link_text='TRENDING')\n",
    "            data = self.parser(src, type='recommended_videos', kind='trending-month')\n",
    "            return data\n",
    "        elif type == 'all':\n",
    "            src = self.call(self.bitchute_base, click_link_text='ALL')\n",
    "            data = self.parser(src, type='recommended_videos', kind='all')\n",
    "            return data\n",
    "        else:\n",
    "            print('Wrong type. Accepted types are popular, trending and all.')\n",
    "            return None\n",
    "        self.reset_webdriver()\n",
    "\n",
    "    def get_popular_videos(self):\n",
    "        videos, tags = self.get_recommended_videos(type='popular')\n",
    "        return videos\n",
    "\n",
    "    def get_trending_videos(self):\n",
    "        videos, tags = self.get_recommended_videos(type='trending')\n",
    "        return videos\n",
    "\n",
    "    def get_trending_tags(self):\n",
    "        videos, tags = self.get_recommended_videos(type='trending')\n",
    "        return tags\n",
    "\n",
    "    def get_trending(self):\n",
    "        videos, tags = self.get_recommended_videos(type='trending')\n",
    "        return videos, tags\n",
    "\n",
    "    def get_all_videos(self):\n",
    "        videos, tags = self.get_recommended_videos(type='all')\n",
    "        return videos\n",
    "\n",
    "    def get_recommended_channels(self, extended=True):\n",
    "        '''\n",
    "        Scapes recommended channels on bitchute homepage.\n",
    "\n",
    "        Parameters:\n",
    "        extended (bool): whether to retrieve extended channel information. Default: True\n",
    "\n",
    "        Returns:\n",
    "        data: Dataframe of recommended channels.\n",
    "        '''\n",
    "        src = self.call(self.bitchute_base, scroll=False)\n",
    "        data = self.parser(src, type='recommended_channels', extended=extended)\n",
    "        self.reset_webdriver()\n",
    "        return data\n",
    "\n",
    "    def _get_channel(self, channel_id, get_channel_about=True, get_channel_videos=True):\n",
    "        '''\n",
    "        Scapes channel information.\n",
    "\n",
    "        Parameters:\n",
    "        channel_id (str): ID of channel to be scraped.\n",
    "        get_channel_about (bool): Get the about information by a channel. Default:True \n",
    "        get_channel_videos (bool): Get the information of videos published by a channel. Default:True\n",
    "\n",
    "        Returns:\n",
    "        about_data: Dataframe of channel about.\n",
    "        videos_data: Dataframe of channel videos.\n",
    "        '''\n",
    "\n",
    "        if get_channel_about:\n",
    "            channel_about_url = self.channel_base.format(channel_id)\n",
    "            src = self.call(channel_about_url, click_link_text='ABOUT', scroll=False)\n",
    "            about_data = self.parser(src, type='channel_about')\n",
    "        else:\n",
    "            about_data = pd.DataFrame()\n",
    "\n",
    "        if get_channel_videos:\n",
    "            channel_videos_url = self.channel_base.format(channel_id)\n",
    "            src = self.call(channel_videos_url, click_link_text='VIDEOS')\n",
    "            videos_data = self.parser(src, type='channel_videos')\n",
    "        else:\n",
    "            videos_data = pd.DataFrame()\n",
    "\n",
    "        return about_data, videos_data\n",
    "\n",
    "    def get_channels(self, channel_ids, get_channel_about=True, get_channel_videos=True):\n",
    "        '''\n",
    "        Scapes information for multiple channels.\n",
    "\n",
    "        Parameters:\n",
    "        channel_ids (list): List of channel ids to be scraped.\n",
    "        get_channel_about (bool): Get the about information by a channel. Default:True \n",
    "        get_channel_videos (bool): Get the information of videos published by a channel. Default:True\n",
    "\n",
    "        Returns:\n",
    "        abouts: Dataframe of channel abouts.\n",
    "        videos: Dataframe of channel videos.\n",
    "        '''\n",
    "        if type(channel_ids) == str:\n",
    "            abouts, videos = self._get_channel(channel_ids, get_channel_about=get_channel_about, get_channel_videos=get_channel_videos)\n",
    "            self.reset_webdriver()\n",
    "            return abouts, videos\n",
    "        elif type(channel_ids) == list:\n",
    "            abouts = pd.DataFrame()\n",
    "            videos = pd.DataFrame()\n",
    "            for channel_id in (tqdm(channel_ids) if not self.verbose else channel_ids):\n",
    "                about_tmp, videos_tmp = self._get_channel(channel_id, get_channel_about=get_channel_about, get_channel_videos=get_channel_videos)\n",
    "                abouts = pd.concat([abouts, about_tmp])\n",
    "                videos = pd.concat([videos, videos_tmp])\n",
    "            self.reset_webdriver()\n",
    "            return abouts, videos\n",
    "        else:\n",
    "            print('channel_ids must be of type list for multiple or str for single channels')\n",
    "            return None\n",
    "\n",
    "    def _get_video(self, video_id):\n",
    "        '''\n",
    "        Scrapes video metadata.\n",
    "\n",
    "        Parameters:\n",
    "        video_id (str): ID of video to be scraped.\n",
    "\n",
    "        Returns:\n",
    "        video_data: Dataframe of video metadata.\n",
    "        '''\n",
    "\n",
    "        video_url = self.video_base + video_id + \"/\"\n",
    "        src = self.call(video_url, scroll=False)\n",
    "        video_data = self.parser(src, type='video')\n",
    "        return video_data\n",
    "\n",
    "    def get_videos(self, video_ids):\n",
    "        '''\n",
    "        Scrapes metadata of multiple videos.\n",
    "\n",
    "        Parameters:\n",
    "        video_ids (list): List of video ids to be scraped.\n",
    "\n",
    "        Returns:\n",
    "        video_data: Dataframe of video metadata.\n",
    "        '''\n",
    "\n",
    "        if type(video_ids) == str:\n",
    "            try:\n",
    "                video_data = self._get_video(video_ids)\n",
    "                self.reset_webdriver()\n",
    "                return video_data\n",
    "            except:\n",
    "                print('Failed for video with id {}'.format(video_ids))\n",
    "        elif type(video_ids) == list:\n",
    "            video_data = pd.DataFrame()\n",
    "            for video_id in (tqdm(video_ids) if not self.verbose else video_ids):\n",
    "                try:\n",
    "                    video_tmp = self._get_video(video_id)\n",
    "                    if video_tmp is not None:\n",
    "                        video_data = pd.concat([video_data, video_tmp])\n",
    "                except:\n",
    "                    print('Failed for video with id {}'.format(video_id))\n",
    "                    self.reset_webdriver()\n",
    "            self.reset_webdriver()\n",
    "            return video_data\n",
    "        else:\n",
    "            print('video_ids must be of type list for multiple or str for single video')\n",
    "            return None\n",
    "\n",
    "    def _get_hashtag(self, hashtag):\n",
    "        '''\n",
    "        Scapes video posted with a tag.\n",
    "\n",
    "        Parameters:\n",
    "        tag (str): Hashtag to be scraped.\n",
    "\n",
    "        Returns:\n",
    "        video_data: Dataframe of video metadata.\n",
    "        '''\n",
    "        hashtag_url = self.hashtag_base.format(hashtag)\n",
    "        src = self.call(hashtag_url)\n",
    "        video_data = self.parser(src, type='hashtag_videos')\n",
    "        video_data['hashtag'] = hashtag\n",
    "        return video_data\n",
    "\n",
    "    def get_hashtags(self, hashtags):\n",
    "        '''\n",
    "        Scapes video posted with a tag.\n",
    "\n",
    "        Parameters:\n",
    "        tag (str): Hashtag to be scraped.\n",
    "\n",
    "        Returns:\n",
    "        video_data: Dataframe of video metadata.\n",
    "        '''\n",
    "\n",
    "        if type(hashtags) == str:\n",
    "            video_data = self._get_hashtag(hashtags)\n",
    "            video_data['hashtag'] = hashtags\n",
    "            self.reset_webdriver()\n",
    "            return video_data\n",
    "        elif type(hashtags) == list:\n",
    "            video_data = pd.DataFrame()\n",
    "            for hashtag in (tqdm(hashtags) if not self.verbose else hashtags):\n",
    "                video_tmp = self._get_hashtag(hashtag)\n",
    "                if video_tmp is not None:\n",
    "                    video_tmp['hashtag'] = hashtag\n",
    "                    video_data = pd.concat([video_data, video_tmp])\n",
    "            self.reset_webdriver()\n",
    "            return video_data\n",
    "        else:\n",
    "            print('hashtags must be of type list for multiple or str for single hashtag')\n",
    "            return None\n",
    "\n",
    "    def convert_relative_time(self, relative_time_str):\n",
    "        now = datetime.utcnow()\n",
    "        relative_time_str = relative_time_str.lower()\n",
    "        if \"hour\" in relative_time_str:\n",
    "            hours = int(relative_time_str.split()[0])\n",
    "            return now - timedelta(hours=hours)\n",
    "        elif \"minute\" in relative_time_str:\n",
    "            minutes = int(relative_time_str.split()[0])\n",
    "            return now - timedelta(minutes=minutes)\n",
    "        elif \"day\" in relative_time_str and \"week\" in relative_time_str:\n",
    "            parts = relative_time_str.split(',')\n",
    "            days = int(parts[1].strip().split()[0])\n",
    "            weeks = int(parts[0].strip().split()[0])\n",
    "            return now - timedelta(days=days, weeks=weeks)\n",
    "        elif \"days\" in relative_time_str:\n",
    "            days = int(relative_time_str.split()[0])\n",
    "            return now - timedelta(days=days)\n",
    "        elif \"week\" in relative_time_str:\n",
    "            weeks = int(relative_time_str.split()[0])\n",
    "            return now - timedelta(weeks=weeks)\n",
    "        else:\n",
    "            return dateutil_parse(relative_time_str)\n",
    "\n",
    "    def get_recent_videos(self, url):\n",
    "        recent_videos = []\n",
    "        src = self.call(url)\n",
    "        videos = self.parser(src, type='video_search')  # Ensure the type matches your use case\n",
    "        for index, row in videos.iterrows():\n",
    "            recent_videos.append(row)\n",
    "        return pd.DataFrame(recent_videos)\n",
    "\n",
    "\n",
    "\n",
    "    def get_video_details(self, video_id):\n",
    "        url = f\"{self.video_base}{video_id}/\"\n",
    "        print(f\"Retrieving: {url}\")  # Debug: Print the URL to check if it's correct\n",
    "        src = self.call(url)\n",
    "        soup = BeautifulSoup(src, 'html.parser')\n",
    "\n",
    "        like_count = None\n",
    "        dislike_count = None\n",
    "        subscriber_count = None\n",
    "        hashtags = []\n",
    "\n",
    "        if soup.find(class_='video-like'):\n",
    "            like_count_text = soup.find(id='video-like-count').text.strip()\n",
    "            try:\n",
    "                like_count = int(like_count_text.replace(',', ''))\n",
    "            except ValueError:\n",
    "                like_count = like_count_text\n",
    "\n",
    "        if soup.find(class_='video-dislike'):\n",
    "            dislike_count_text = soup.find(id='video-dislike-count').text.strip()\n",
    "            try:\n",
    "                dislike_count = int(dislike_count_text.replace(',', ''))\n",
    "            except ValueError:\n",
    "                dislike_count = dislike_count_text\n",
    "\n",
    "        if soup.find(class_='subscriber-count'):\n",
    "            subscriber_count_text = soup.find(id='subscriber_count').text.strip()\n",
    "            try:\n",
    "                subscriber_count = int(subscriber_count_text.replace(',', ''))\n",
    "            except ValueError:\n",
    "                subscriber_count = subscriber_count_text\n",
    "\n",
    "        # Find video hashtags\n",
    "        video_hashtags_element = soup.find(id='video-hashtags')\n",
    "        if video_hashtags_element:\n",
    "            for tag in video_hashtags_element.find_all('li'):\n",
    "                hashtags.append(tag.text.strip())\n",
    "\n",
    "        return like_count, dislike_count, subscriber_count, hashtags\n",
    "\n",
    "    def process_likes(self, likes):\n",
    "        try:\n",
    "            likes = likes.replace(',', '').strip()  # Remove commas and strip whitespace\n",
    "            return int(likes)\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    def process_dislikes(self, dislikes):\n",
    "        try:\n",
    "            dislikes = dislikes.replace(',', '').strip()  # Remove commas and strip whitespace\n",
    "            return int(dislikes)\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    def process_subscribers(self, subscribers):\n",
    "        try:\n",
    "            subscribers = subscribers.replace(',', '').strip()  # Remove commas and strip whitespace\n",
    "            return int(subscribers)\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    def parser(self, src, type=None, kind=None, extended=False):\n",
    "        scrape_time = str(int(datetime.utcnow().timestamp()))\n",
    "\n",
    "        soup = BeautifulSoup(src, 'html.parser')\n",
    "        if soup.find('h1') and (\"404 - Page not found\" in soup.find('h1').text or \"404 - PAGE NOT FOUND\" in soup.find('h1').text):\n",
    "            return None\n",
    "\n",
    "        if not type:\n",
    "            raise 'A parse type needs to be passed.'\n",
    "\n",
    "        if type == 'video_search' or type == 'hashtag_videos':\n",
    "            videos = []\n",
    "            if soup.find(class_='results-list'):\n",
    "                counter = 0\n",
    "                for result in soup.find_all(class_='video-result-container'):\n",
    "                    counter += 1\n",
    "                    title = None\n",
    "                    id_ = None\n",
    "                    view_count = None\n",
    "                    duration = None\n",
    "                    channel = None\n",
    "                    channel_id = None\n",
    "                    description = None\n",
    "                    description_links = []\n",
    "                    created_at = None\n",
    "                    like_count = None\n",
    "                    dislike_count = None\n",
    "                    subscriber_count = None\n",
    "                    hashtags = []\n",
    "\n",
    "                    if result.find(class_='video-result-title'):\n",
    "                        title = result.find(class_='video-result-title').text.strip('\\n').strip()\n",
    "                        id_ = result.find(class_='video-result-title').find('a').get('href').split('/')[-2]\n",
    "\n",
    "                    if result.find(class_='video-views'):\n",
    "                        view_count = self.process_views(result.find(class_='video-views').text.strip('\\n').strip())\n",
    "\n",
    "                    if result.find(class_='video-duration'):\n",
    "                        duration = result.find(class_='video-duration').text.strip('\\n').strip()\n",
    "\n",
    "                    if result.find(class_='video-result-channel'):\n",
    "                        channel = result.find(class_='video-result-channel').text.strip('\\n').strip()\n",
    "                        channel_id = result.find(class_='video-result-channel').find('a').get('href').split('/')[-2]\n",
    "\n",
    "                    if result.find(class_='video-result-text'):\n",
    "                        description = result.find(class_='video-result-text').decode_contents()\n",
    "                        description = description.strip('\\n')\n",
    "                        description = markdownify.markdownify(description)\n",
    "\n",
    "                        for link in result.find(class_='video-result-text').find_all('a'):\n",
    "                            description_links.append(link.get('href'))\n",
    "\n",
    "                    if result.find(class_='video-result-details'):\n",
    "                        created_at = result.find(class_='video-result-details').text.strip('\\n').strip()\n",
    "\n",
    "                    if result.find(class_='video-like'):\n",
    "                        like_count_text = result.find(class_='video-like').find(id='video-like-count').text.strip()\n",
    "                        like_count = like_count_text\n",
    "                        print(f\"Scraped like count: {like_count}\")\n",
    "\n",
    "                    if result.find(class_='video-dislike'):\n",
    "                        dislike_count_text = result.find(class_='video-dislike').find(id='video-dislike-count').text.strip()\n",
    "                        dislike_count = dislike_count_text\n",
    "                        print(f\"Scraped dislike count: {dislike_count}\")\n",
    "\n",
    "                    if result.find(class_='subscriber-count'):\n",
    "                        subscriber_count_text = result.find(class_='subscriber-count').find(id='subscriber_count').text.strip()\n",
    "                        subscriber_count = subscriber_count_text\n",
    "                        print(f\"Scraped subscriber count: {subscriber_count}\")\n",
    "\n",
    "                    if result.find(id='video-hashtags'):\n",
    "                        video_hashtags_element = result.find(id='video-hashtags')\n",
    "                        if video_hashtags_element:\n",
    "                            for tag in video_hashtags_element.find_all('li'):\n",
    "                                hashtags.append(tag.text.strip())\n",
    "\n",
    "                    videos.append([counter, id_, title, hashtags, view_count, duration, channel, channel_id, description, description_links, created_at, scrape_time])\n",
    "\n",
    "            videos_columns = ['rank', 'id', 'title', 'hashtags', 'view_count', 'duration', 'channel', 'channel_id', 'description', 'description_links', 'created_at', 'scrape_time']\n",
    "            videos = pd.DataFrame(videos, columns=videos_columns)\n",
    "            return videos\n",
    "\n",
    "        elif type == 'recommended_channels':\n",
    "            channels = []\n",
    "            channel_ids = []\n",
    "            soup = BeautifulSoup(src, 'html.parser')\n",
    "            counter = 0\n",
    "            if soup.find(id='carousel'):\n",
    "                for item in soup.find(id='carousel').find_all(class_='channel-card'):\n",
    "                    counter += 1\n",
    "                    id_ = item.find('a').get('href').split('/')[-2]\n",
    "                    name = item.find(class_='channel-card-title').text\n",
    "                    channels.append([counter, id_, name, scrape_time])\n",
    "                    channel_ids.append(id_)\n",
    "            if extended:\n",
    "                channel_ids = list(set(channel_ids))\n",
    "                channels, videos = self.get_channels(channel_ids, get_channel_videos=False)\n",
    "            else:\n",
    "                columns = ['rank', 'id', 'name', 'scrape_time']\n",
    "                channels = pd.DataFrame(channels, columns=columns)\n",
    "                channels = channels.drop_duplicates(subset=['id'])\n",
    "            return channels\n",
    "\n",
    "        elif type == 'recommended_videos':\n",
    "            videos = []\n",
    "            tags = []\n",
    "            soup = BeautifulSoup(src, 'html.parser')\n",
    "\n",
    "            if kind == 'popular':\n",
    "                if soup.find(id='listing-popular'):\n",
    "                    soup = soup.find(id='listing-popular')\n",
    "                else:\n",
    "                    return None\n",
    "\n",
    "            elif kind == 'trending-day':\n",
    "                if soup.find(id='trending-day'):\n",
    "                    soup = soup.find(id='trending-day')\n",
    "                else:\n",
    "                    return None\n",
    "\n",
    "            elif kind == 'trending-week':\n",
    "                if soup.find(id='trending-week'):\n",
    "                    soup = soup.find(id='trending-week')\n",
    "                else:\n",
    "                    return None\n",
    "\n",
    "            elif kind == 'trending-month':\n",
    "                if soup.find(id='trending-month'):\n",
    "                    soup = soup.find(id='trending-month')\n",
    "                else:\n",
    "                    return None\n",
    "\n",
    "            elif kind == 'all':\n",
    "                if soup.find(id='listing-all'):\n",
    "                    soup = soup.find(id='listing-all')\n",
    "                else:\n",
    "                    return None\n",
    "            else:\n",
    "                print('kind needs to be passed for recommendations.')\n",
    "                return None\n",
    "\n",
    "            if soup.find(class_='video-result-container'):\n",
    "                counter = 0\n",
    "                for video in soup.find_all(class_='video-result-container'):\n",
    "                    counter += 1\n",
    "                    title = None\n",
    "                    id_ = None\n",
    "                    view_count = None\n",
    "                    duration = None\n",
    "                    channel = None\n",
    "                    channel_url = None\n",
    "                    created_at = None\n",
    "\n",
    "                    if video.find(class_='video-result-title'):\n",
    "                        title = video.find(class_='video-result-title').text.strip('\\n')\n",
    "                        id_ = video.find(class_='video-result-title').find('a').get('href').split('/')[-2]\n",
    "\n",
    "                    if video.find(class_='video-views'):\n",
    "                        view_count = self.process_views(video.find(class_='video-views').text.strip('\\n'))\n",
    "                    if video.find(class_='video-duration'):\n",
    "                        duration = video.find(class_='video-duration').text.strip('\\n')\n",
    "\n",
    "                    if video.find(class_='video-result-channel'):\n",
    "                        channel = video.find(class_='video-result-channel').text.strip('\\n')\n",
    "                        channel_id = video.find(class_='video-result-channel').find('a').get('href').split('/')[-2]\n",
    "                    if video.find(class_='video-result-details'):\n",
    "                        created_at = video.find(class_='video-result-details').text.strip('\\n')\n",
    "                    videos.append([counter, id_, title, view_count, duration, channel, channel_id, created_at, scrape_time])\n",
    "\n",
    "            elif soup.find(class_='video-card'):\n",
    "                counter = 0\n",
    "                for video in soup.find_all(class_='video-card'):\n",
    "                    counter += 1\n",
    "                    title = None\n",
    "                    id_ = None\n",
    "                    view_count = None\n",
    "                    duration = None\n",
    "                    channel = None\n",
    "                    channel_url = None\n",
    "                    created_at = None\n",
    "\n",
    "                    if video.find(class_='video-card-title'):\n",
    "                        title = video.find(class_='video-card-title').text.strip('\\n').strip()\n",
    "                    if video.find(class_='video-card-id'):\n",
    "                        id_ = video.find(class_='video-card-id').text.strip('\\n').strip()\n",
    "                    if video.find(class_='video-views'):\n",
    "                        view_count = self.process_views(video.find(class_='video-views').text.strip('\\n').strip())\n",
    "                    if video.find(class_='video-duration'):\n",
    "                        duration = video.find(class_='video-duration').text.strip('\\n').strip()\n",
    "                    if video.find(class_='video-card-channel'):\n",
    "                        channel = video.find(class_='video-card-channel').text.strip('\\n').strip()\n",
    "                        channel_id = video.find(class_='video-card-channel').find('a').get('href').split('/')[-2]\n",
    "                    if video.find(class_='video-card-published'):\n",
    "                        created_at = video.find(class_='video-card-published').text.strip('\\n').strip()\n",
    "                    videos.append([counter, id_, title, view_count, duration, channel, channel_id, created_at, scrape_time])\n",
    "\n",
    "            soup = BeautifulSoup(src, 'html.parser')\n",
    "            if soup.find(class_='sidebar tags'):\n",
    "                counter = 0\n",
    "                for tag in soup.find(class_='sidebar tags').find_all('li'):\n",
    "                    counter += 1\n",
    "                    tag_name = tag.text.strip('\\n').strip()\n",
    "                    tag_url = tag.find('a').get('href')\n",
    "                    tags.append([counter, tag_name, tag_url, scrape_time])\n",
    "\n",
    "            videos_columns = ['rank', 'id', 'title', 'view_count', 'duration', 'channel', 'channel_id', 'created_at', 'scrape_time']\n",
    "            videos = pd.DataFrame(videos, columns=videos_columns)\n",
    "\n",
    "            tags_columns = ['rank', 'tag_name', 'tag_url', 'scrape_time']\n",
    "            tags = pd.DataFrame(tags, columns=tags_columns)\n",
    "\n",
    "            return videos, tags\n",
    "\n",
    "        elif type == 'channel_about':\n",
    "            uid = None\n",
    "            id_ = None\n",
    "            title = None\n",
    "            owner = None\n",
    "            owner_link = None\n",
    "            description = None\n",
    "            description_links = []\n",
    "            social_links = []\n",
    "            category = None\n",
    "            video_count = None\n",
    "            subscriber_count = None\n",
    "            view_count = None\n",
    "            created_at = None\n",
    "\n",
    "            soup = BeautifulSoup(src, 'html.parser')\n",
    "            if soup.find('link', id='canonical'):\n",
    "                uid = soup.find('link', id='canonical').get('href').split('/')[-2]\n",
    "            if soup.find(class_='name'):\n",
    "                title = soup.find(class_='name').text.strip('\\n').strip()\n",
    "                if soup.find(class_='name').find('a'):\n",
    "                    id_ = soup.find(class_='name').find('a').get('href').strip('/').split('/')[1]\n",
    "            if soup.find(class_='owner'):\n",
    "                owner = soup.find(class_='owner').text.strip('\\n').strip()\n",
    "                owner_link = soup.find(class_='owner').find('a').get('href')\n",
    "            if soup.find(id='channel-description'):\n",
    "                description = soup.find(id='channel-description').decode_contents()\n",
    "                description = description.strip('\\n')\n",
    "                description = markdownify.markdownify(description)\n",
    "                for link in soup.find(id='channel-description').find_all('a'):\n",
    "                    description_links.append(link.get('href'))\n",
    "            if soup.find(class_='social'):\n",
    "                for link in soup.find(class_='social').find_all('a'):\n",
    "                    social_links.append([link.get('data-original-title'), link.get('href')])\n",
    "            if soup.find(class_='channel-about-details'):\n",
    "                for elem in soup.find(class_='channel-about-details').find_all('p'):\n",
    "                    if 'Category' in elem.text and elem.find('a'):\n",
    "                        category = elem.find('a').text.strip('\\n').strip()\n",
    "                    elif elem.find(class_='fa-video'):\n",
    "                        video_count = elem.text.split(' ')[1]\n",
    "                    elif elem.find(class_='fa-users'):\n",
    "                        subscriber_count = int(elem.text.split(' ')[1].replace(',', ''))\n",
    "                        print(f\"Subscriber count: {subscriber_count}\")  # Debug statement\n",
    "                    elif elem.find(class_='fa-eye'):\n",
    "                        view_count = self.process_views(elem.text.split(' ')[1])\n",
    "                    else:\n",
    "                        created_at = elem.text.strip('\\n').strip()\n",
    "                        pass\n",
    "            data = [uid, id_, title, description, description_links, video_count, subscriber_count, view_count, created_at, category, social_links, owner, owner_link, scrape_time]\n",
    "            columns = ['uid', 'id', 'title', 'description', 'description_links', 'video_count', 'subscriber_count', 'view_count', 'created_at', 'category', 'social_links', 'owner', 'owner_link', 'scrape_time']\n",
    "            data = pd.DataFrame([data], columns=columns)\n",
    "            return data\n",
    "\n",
    "        elif type == 'channel_videos':\n",
    "            soup = BeautifulSoup(src, 'html.parser')\n",
    "            data = []\n",
    "\n",
    "            if soup.find('link', id='canonical').get('href'):\n",
    "                channel_id = soup.find('link', id='canonical').get('href').split('/')[-2]\n",
    "            else:\n",
    "                channel_id = None\n",
    "            if soup.find(class_='name'):\n",
    "                channel_title = soup.find(class_='name').text.strip('\\n')\n",
    "            else:\n",
    "                channel_title = None\n",
    "            if soup.find(class_='channel-videos-list'):\n",
    "                for video in soup.find(class_='channel-videos-list').find_all(class_='channel-videos-container'):\n",
    "                    if video.find(class_='channel-videos-title'):\n",
    "                        title = video.find(class_='channel-videos-title').text.strip('\\n')\n",
    "                        video_id = video.find(class_='channel-videos-title').find('a').get('href').split('/')[-2]\n",
    "                    else:\n",
    "                        title = None\n",
    "                        video_id = None\n",
    "                    if video.find(class_='channel-videos-text'):\n",
    "                        description = video.find(class_='channel-videos-text').decode_contents()\n",
    "                        description = description.strip('\\n')\n",
    "                        description = markdownify.markdownify(description)\n",
    "                        description_links = [a.get('href') for a in video.find(class_='channel-videos-text').find_all('a')]\n",
    "                    else:\n",
    "                        description = None\n",
    "                        description_links = []\n",
    "                    if video.find(class_='video-duration'):\n",
    "                        duration = video.find(class_='video-duration').text.strip('\\n').strip()\n",
    "                    else:\n",
    "                        duration = None\n",
    "                    if video.find(class_='channel-videos-details'):\n",
    "                        created_at = str(parser.parse(video.find(class_='channel-videos-details').text.replace('\\n', '')).date())\n",
    "                    else:\n",
    "                        created_at = None\n",
    "                    if video.find(class_='video-views'):\n",
    "                        view_count = self.process_views(video.find(class_='video-views').text.strip('\\n').strip())\n",
    "                    else:\n",
    "                        view_count = None\n",
    "\n",
    "                    data.append([channel_id, channel_title, video_id, title, created_at, duration, view_count, description, description_links, scrape_time])\n",
    "\n",
    "            columns = ['channel_id', 'channel_title', 'video_id', 'title', 'created', 'duration', 'view_count', 'description', 'description_links', 'scrape_time']\n",
    "            data = pd.DataFrame(data, columns=columns)\n",
    "            return data\n",
    "\n",
    "        elif type == 'video':\n",
    "            id_ = None\n",
    "            title = None\n",
    "            description = None\n",
    "            description_links = []\n",
    "            view_count = None\n",
    "            like_count = None\n",
    "            dislike_count = None\n",
    "            created_at = None\n",
    "            hashtags = []\n",
    "            category = None\n",
    "            sensitivity = None\n",
    "            channel_name = None\n",
    "            channel_id = None\n",
    "            owner_name = None\n",
    "            owner_id = None\n",
    "            subscribers = None\n",
    "            next_id = None\n",
    "            related_ids = []\n",
    "\n",
    "            if soup.find(id='canonical'):\n",
    "                id_ = soup.find(id='canonical').get('href').split('/')[-2]\n",
    "            if soup.find(id='video-title'):\n",
    "                title = soup.find(id='video-title').text.strip('\\n').strip()\n",
    "            if soup.find(id='video-view-count'):\n",
    "                view_count = self.process_views(soup.find(id='video-view-count').text.strip('\\n').strip())\n",
    "                print(f\"View count found: {view_count}\")\n",
    "            else:\n",
    "                print(\"View count element not found.\")\n",
    "            if soup.find(id='video-like-count'):\n",
    "                like_count_text = soup.find(id='video-like-count').text.strip()\n",
    "                print(f\"Like count text: {like_count_text}\")\n",
    "                try:\n",
    "                    like_count = int(like_count_text.replace(',', ''))\n",
    "                    print(f\"Like count converted: {like_count}\")\n",
    "                except ValueError:\n",
    "                    print(\"Error converting like count\")\n",
    "            else:\n",
    "                print(\"Like count element not found.\")\n",
    "            if soup.find(id='video-dislike-count'):\n",
    "                dislike_count_text = soup.find(id='video-dislike-count').text.strip()\n",
    "                print(f\"Dislike count text: {dislike_count_text}\")\n",
    "                try:\n",
    "                    dislike_count = int(dislike_count_text.replace(',', ''))\n",
    "                    print(f\"Dislike count converted: {dislike_count}\")\n",
    "                except ValueError:\n",
    "                    print(\"Error converting dislike count\")\n",
    "            else:\n",
    "                print(\"Dislike count element not found.\")\n",
    "            if soup.find(class_='video-publish-date'):\n",
    "                created_at = soup.find(class_='video-publish-date').text.strip().replace('First published at ', '')\n",
    "                created_at = parser.parse(created_at)\n",
    "\n",
    "            if soup.find(id='video-hashtags'):\n",
    "                if soup.find(id='video-hashtags').find('li'):\n",
    "                    for tag in soup.find(id='video-hashtags').find_all('li'):\n",
    "                        hashtags.append(tag.text.strip())\n",
    "            if soup.find(id='video-description'):\n",
    "                description = soup.find(id='video-description').decode_contents()\n",
    "                description = description.strip()\n",
    "                description = markdownify.markdownify(description)\n",
    "                if soup.find(id='video-description').find('a'):\n",
    "                    for link in soup.find(id='video-description').find_all('a'):\n",
    "                        description_links.append(link.get('href'))\n",
    "            if soup.find(class_='video-detail-list'):\n",
    "                if soup.find(class_='video-detail-list').find('tr'):\n",
    "                    for row in soup.find(class_='video-detail-list').find_all('tr'):\n",
    "                        value = row.find('a').text\n",
    "                        if 'Category' in row.text:\n",
    "                            category = value\n",
    "                        elif 'Sensitivity' in row.text:\n",
    "                            sensitivity = value\n",
    "            if soup.find(class_='channel-banner'):\n",
    "                channel_data = soup.find(class_='channel-banner')\n",
    "                if channel_data.find(class_='name'):\n",
    "                    channel_name = channel_data.find(class_='name').text.strip()\n",
    "                    channel_id = channel_data.find(class_='name').find('a').get('href').split('/')[-2]\n",
    "                if channel_data.find(class_='owner'):\n",
    "                    owner_name = channel_data.find(class_='owner').text.strip()\n",
    "                    owner_id = channel_data.find(class_='owner').find('a').get('href').split('/')[-2]\n",
    "                if channel_data.find(class_='subscribers'):\n",
    "                    subscribers_text = channel_data.find(class_='subscribers').text.replace('subscribers', '').strip()\n",
    "                    print(f\"Subscriber count text: {subscribers_text}\")\n",
    "                    try:\n",
    "                        subscribers = int(subscribers_text.replace(',', ''))\n",
    "                        print(f\"Subscriber count converted: {subscribers}\")\n",
    "                    except ValueError:\n",
    "                        print(\"Error converting subscriber count\")\n",
    "\n",
    "            if soup.find(class_='sidebar-next'):\n",
    "                if soup.find(class_='sidebar-next').find(class_='video-card-title'):\n",
    "                    next_id = soup.find(class_='sidebar-next').find(class_='video-card-title').find('a').get('href').split('/')[-2]\n",
    "\n",
    "            if soup.find(class_='sidebar-recent'):\n",
    "                if soup.find(class_='sidebar-recent').find(class_='video-card-title'):\n",
    "                    for item in soup.find(class_='sidebar-recent').find_all(class_='video-card-title'):\n",
    "                        related_ids.append(item.find('a').get('href').split('/')[-2])\n",
    "\n",
    "            columns = ['id', 'title', 'description', 'description_links', 'view_count', 'like_count', 'dislike_count', 'created', 'hashtags', 'category', 'sensitivity', 'channel_name', 'channel_id', 'owner_name', 'owner_id', 'subscribers', 'next_id', 'related_ids']\n",
    "            data = pd.DataFrame([[id_, title, description, description_links, view_count, like_count, dislike_count, created_at, hashtags, category, sensitivity, channel_name, channel_id, owner_name, owner_id, subscribers, next_id, related_ids]], columns=columns)\n",
    "            return data\n",
    "\n",
    "        else:\n",
    "            print('A correct type needs to be passed.')\n",
    "\n",
    "    def get_status(self, reset=True):\n",
    "        status = self.status\n",
    "        if reset:\n",
    "            self.status\n",
    "        return status\n",
    "\n",
    "    def set_status(self, message):\n",
    "        self.status.append(message)\n",
    "\n",
    "def main():\n",
    "    # Create an instance of the Crawler\n",
    "    crawler = Crawler(headless=True, verbose=True)\n",
    "    \n",
    "    # List of categories to scrape\n",
    "    categories = [\n",
    "        ('News', 'https://api.bitchute.com/category/news/'),\n",
    "        ('Health', 'https://api.bitchute.com/category/health/'),\n",
    "        ('Entertainment', 'https://api.bitchute.com/category/entertainment/'),\n",
    "        ('Business & Finance', 'https://api.bitchute.com/category/finance/'),\n",
    "        ('Auto & Vehicles', 'https://api.bitchute.com/category/vehicles/'),\n",
    "        ('Education', 'https://api.bitchute.com/category/education/'),\n",
    "        ('Sports', 'https://api.bitchute.com/category/sport/')\n",
    "    ]\n",
    "\n",
    "    # Directory to save the CSV files\n",
    "    save_directory = r\"C:\\Users\\Shlok Mandloi\\Desktop\\Shlok\\Shlok - USA\\simppl\\dash dashboard\\data\"\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "    for category_name, category_url in categories:\n",
    "        # Get recent videos for the current category\n",
    "        recent_videos = crawler.get_recent_videos(category_url)\n",
    "        \n",
    "        # For each video, get detailed information\n",
    "        for index, row in recent_videos.iterrows():\n",
    "            video_id = row['id']\n",
    "            like_count, dislike_count, subscriber_count, hashtags = crawler.get_video_details(video_id)\n",
    "            recent_videos.at[index, 'like_count'] = like_count\n",
    "            recent_videos.at[index, 'dislike_count'] = dislike_count\n",
    "            recent_videos.at[index, 'subscriber_count'] = subscriber_count\n",
    "            recent_videos.at[index, 'hashtags'] = ','.join(hashtags)\n",
    "        \n",
    "        # Save the results to a CSV file for the current category\n",
    "        file_name = f'recent_videos_bitchute_{category_name}.csv'\n",
    "        file_path = os.path.join(save_directory, file_name)\n",
    "        recent_videos.to_csv(file_path, index=False)\n",
    "        print(f\"Scraping completed for {category_name}. Results saved to '{file_path}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a527e0-b815-4bdc-83bb-2a644fb69c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Directory where CSV files are saved\n",
    "save_directory = r\"C:\\Users\\Shlok Mandloi\\Desktop\\Shlok\\Shlok - USA\\simppl\\dash dashboard\\data\"\n",
    "\n",
    "# Paths to the CSV files\n",
    "csv_files = {\n",
    "    'Sports': 'recent_videos_bitchute_Sports.csv',\n",
    "    'Health': 'recent_videos_bitchute_Health.csv',\n",
    "    'Entertainment': 'recent_videos_bitchute_Entertainment.csv',\n",
    "    'Education': 'recent_videos_bitchute_Education.csv',\n",
    "    'Auto & Vehicles': 'recent_videos_bitchute_Auto & Vehicles.csv',\n",
    "    'Business & Finance': 'recent_videos_bitchute_Business & Finance.csv',\n",
    "    'News': 'recent_videos_bitchute_News.csv'\n",
    "}\n",
    "\n",
    "# Load the data\n",
    "alldata = []\n",
    "for category, file_name in csv_files.items():\n",
    "    file_path = os.path.join(save_directory, file_name)\n",
    "    data = pd.read_csv(file_path)\n",
    "    alldata.append(data)\n",
    "\n",
    "# Copy the data for processing\n",
    "ad = alldata.copy()\n",
    "\n",
    "# Print information about each DataFrame\n",
    "for data in ad:\n",
    "    print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad51203a-7bb1-48aa-8481-4ac75bc761ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "86455114-219a-4755-adba-c2068312b18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved preprocessed data to dash_csv_sports.csv\n",
      "Saved preprocessed data to dash_csv_health.csv\n",
      "Saved preprocessed data to dash_csv_entertainment.csv\n",
      "Saved preprocessed data to dash_csv_education.csv\n",
      "Saved preprocessed data to dash_csv_automotive.csv\n",
      "Saved preprocessed data to dash_csv_business.csv\n",
      "Saved preprocessed data to dash_csv_news.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>view_count</th>\n",
       "      <th>duration</th>\n",
       "      <th>channel</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>description</th>\n",
       "      <th>description_links</th>\n",
       "      <th>created_at</th>\n",
       "      <th>like_count</th>\n",
       "      <th>dislike_count</th>\n",
       "      <th>subscriber_count</th>\n",
       "      <th>hours ago posted</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>video_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Z0D9JccIh7FV</td>\n",
       "      <td>She told him he could tie her up and do anythi...</td>\n",
       "      <td>No Hashtags</td>\n",
       "      <td>6335</td>\n",
       "      <td>0:17</td>\n",
       "      <td>PIRATEPETE</td>\n",
       "      <td>piratepete</td>\n",
       "      <td>mirrored from rumble  \\nIMO this guy has his p...</td>\n",
       "      <td>['http://old.bitchute.com']</td>\n",
       "      <td>19hours ago</td>\n",
       "      <td>108</td>\n",
       "      <td>21</td>\n",
       "      <td>18149</td>\n",
       "      <td>19</td>\n",
       "      <td>6464</td>\n",
       "      <td>https://api.bitchute.com/video/Z0D9JccIh7FV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H7XEkx0Ro2OG</td>\n",
       "      <td>CHARLIE WARD DAILY NEWS WITH PAUL BROOKER &amp; DR...</td>\n",
       "      <td>#charlieward,#dailynews,#breaking</td>\n",
       "      <td>4623</td>\n",
       "      <td>17:11</td>\n",
       "      <td>Dr Charlie Ward </td>\n",
       "      <td>drcharlieward</td>\n",
       "      <td>Thinking of buying gold or switching your 401k...</td>\n",
       "      <td>['http://www.goldbusters.co.uk/', 'http://www....</td>\n",
       "      <td>10hours ago</td>\n",
       "      <td>157</td>\n",
       "      <td>14</td>\n",
       "      <td>137911</td>\n",
       "      <td>10</td>\n",
       "      <td>4794</td>\n",
       "      <td>https://api.bitchute.com/video/H7XEkx0Ro2OG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GAWFNng7zai1</td>\n",
       "      <td>''We are ALL in trouble'' - Edward Snowden ......</td>\n",
       "      <td>No Hashtags</td>\n",
       "      <td>2748</td>\n",
       "      <td>8:07</td>\n",
       "      <td>XANDREWX</td>\n",
       "      <td>xandrewx</td>\n",
       "      <td>No Description</td>\n",
       "      <td>[]</td>\n",
       "      <td>23hours ago</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>33721</td>\n",
       "      <td>23</td>\n",
       "      <td>2776</td>\n",
       "      <td>https://api.bitchute.com/video/GAWFNng7zai1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G0XNXJOIqrM3</td>\n",
       "      <td>RESET MAPS [2023-11-07] - CONSPIRACY-R-US (VIDEO)</td>\n",
       "      <td>No Hashtags</td>\n",
       "      <td>1610</td>\n",
       "      <td>10:23</td>\n",
       "      <td>Sergeant Major</td>\n",
       "      <td>sergeant-major</td>\n",
       "      <td>Looking into interesting maps of the Land Down...</td>\n",
       "      <td>['https://www.bitchute.com/video/xGIHac7GvY2K/...</td>\n",
       "      <td>15hours ago</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>60726</td>\n",
       "      <td>15</td>\n",
       "      <td>1643</td>\n",
       "      <td>https://api.bitchute.com/video/G0XNXJOIqrM3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rF0_ZqFvf34</td>\n",
       "      <td>The Battle Against Parasites</td>\n",
       "      <td>No Hashtags</td>\n",
       "      <td>812</td>\n",
       "      <td>0:44</td>\n",
       "      <td>Jordan Peterson</td>\n",
       "      <td>jordanpeterson</td>\n",
       "      <td>No Description</td>\n",
       "      <td>[]</td>\n",
       "      <td>17hours ago</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>18947</td>\n",
       "      <td>17</td>\n",
       "      <td>841</td>\n",
       "      <td>https://api.bitchute.com/video/rF0_ZqFvf34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                              title  \\\n",
       "0  Z0D9JccIh7FV  She told him he could tie her up and do anythi...   \n",
       "1  H7XEkx0Ro2OG  CHARLIE WARD DAILY NEWS WITH PAUL BROOKER & DR...   \n",
       "2  GAWFNng7zai1  ''We are ALL in trouble'' - Edward Snowden ......   \n",
       "3  G0XNXJOIqrM3  RESET MAPS [2023-11-07] - CONSPIRACY-R-US (VIDEO)   \n",
       "4   rF0_ZqFvf34                       The Battle Against Parasites   \n",
       "\n",
       "                            hashtags  view_count duration  \\\n",
       "0                        No Hashtags        6335     0:17   \n",
       "1  #charlieward,#dailynews,#breaking        4623    17:11   \n",
       "2                        No Hashtags        2748     8:07   \n",
       "3                        No Hashtags        1610    10:23   \n",
       "4                        No Hashtags         812     0:44   \n",
       "\n",
       "               channel      channel_id  \\\n",
       "0           PIRATEPETE      piratepete   \n",
       "1  Dr Charlie Ward    drcharlieward   \n",
       "2             XANDREWX        xandrewx   \n",
       "3       Sergeant Major  sergeant-major   \n",
       "4      Jordan Peterson  jordanpeterson   \n",
       "\n",
       "                                         description  \\\n",
       "0  mirrored from rumble  \\nIMO this guy has his p...   \n",
       "1  Thinking of buying gold or switching your 401k...   \n",
       "2                                     No Description   \n",
       "3  Looking into interesting maps of the Land Down...   \n",
       "4                                     No Description   \n",
       "\n",
       "                                   description_links    created_at  \\\n",
       "0                        ['http://old.bitchute.com']  19hours ago   \n",
       "1  ['http://www.goldbusters.co.uk/', 'http://www....  10hours ago   \n",
       "2                                                 []  23hours ago   \n",
       "3  ['https://www.bitchute.com/video/xGIHac7GvY2K/...  15hours ago   \n",
       "4                                                 []  17hours ago   \n",
       "\n",
       "   like_count  dislike_count  subscriber_count  hours ago posted  \\\n",
       "0         108             21             18149                19   \n",
       "1         157             14            137911                10   \n",
       "2          15             13             33721                23   \n",
       "3          31              2             60726                15   \n",
       "4          13             16             18947                17   \n",
       "\n",
       "   total_interactions                                    video_url  \n",
       "0                6464  https://api.bitchute.com/video/Z0D9JccIh7FV  \n",
       "1                4794  https://api.bitchute.com/video/H7XEkx0Ro2OG  \n",
       "2                2776  https://api.bitchute.com/video/GAWFNng7zai1  \n",
       "3                1643  https://api.bitchute.com/video/G0XNXJOIqrM3  \n",
       "4                 841   https://api.bitchute.com/video/rF0_ZqFvf34  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Directory where the cleaned CSV files will be saved\n",
    "save_directory = r\"C:\\Users\\Shlok Mandloi\\Desktop\\Shlok\\Shlok - USA\\simppl\\dash dashboard\\data\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Improved preprocess function to handle relative times in 'created_at'\n",
    "def preprocess_data(df):\n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(columns=['rank', 'scrape_time'])\n",
    "    \n",
    "    # Fill missing values\n",
    "    df['hashtags'] = df['hashtags'].fillna('No Hashtags')\n",
    "    df['description'] = df['description'].fillna('No Description')\n",
    "    \n",
    "    # Function to convert relative time strings to total hours\n",
    "    def convert_relative_time_to_hours(relative_time_str):\n",
    "        if pd.isna(relative_time_str):\n",
    "            return None\n",
    "        try:\n",
    "            # Remove commas and non-breaking spaces\n",
    "            relative_time_str = relative_time_str.replace(',', '').replace('', '').strip()\n",
    "            # Regular expression to match time units\n",
    "            pattern = re.compile(r'(\\d+)\\s*(hour|minute|day|week)s?', re.IGNORECASE)\n",
    "            matches = pattern.findall(relative_time_str)\n",
    "            total_hours = 0\n",
    "            for number, unit in matches:\n",
    "                number = int(number)\n",
    "                if 'hour' in unit:\n",
    "                    total_hours += number\n",
    "                elif 'minute' in unit:\n",
    "                    total_hours += number / 60\n",
    "                elif 'day' in unit:\n",
    "                    total_hours += number * 24\n",
    "                elif 'week' in unit:\n",
    "                    total_hours += number * 24 * 7\n",
    "            return total_hours\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting time: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Convert relative 'created_at' to total hours\n",
    "    df['hours ago posted'] = df['created_at'].apply(convert_relative_time_to_hours)\n",
    "    \n",
    "    # Normalize numerical columns (if needed)\n",
    "    df['view_count'] = df['view_count'].fillna(0).astype(int)\n",
    "    df['like_count'] = df['like_count'].fillna(0).astype(int)\n",
    "    df['dislike_count'] = df['dislike_count'].fillna(0).astype(int)\n",
    "    df['subscriber_count'] = df['subscriber_count'].fillna(0).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to add total interactions column\n",
    "def add_total_interactions(df):\n",
    "    df['total_interactions'] = df['like_count'] + df['dislike_count'] + df['view_count']\n",
    "    return df\n",
    "\n",
    "# Function to remove duplicates based on video ID\n",
    "def remove_duplicates(df):\n",
    "    if 'id' in df.columns:\n",
    "        df = df.drop_duplicates(subset='id')\n",
    "    return df\n",
    "\n",
    "# Function to add a URL column based on video ID\n",
    "def add_video_url(df):\n",
    "    if 'id' in df.columns:\n",
    "        df['video_url'] = df['id'].apply(lambda x: f\"https://api.bitchute.com/video/{x}\")\n",
    "    return df\n",
    "\n",
    "# Assuming `ad` list already has the DataFrames and categories\n",
    "ad = [data_sports, data_health, data_ent, data_edu, data_auto, data_bus, data_news]\n",
    "categories = ['Sports', 'Health', 'Entertainment', 'Education', 'Auto & Vehicles', 'Business & Finance', 'News']\n",
    "\n",
    "# Initialize an empty list to store cleaned dataframes\n",
    "cleaned_dataframes = []\n",
    "\n",
    "# Iterate through each dataframe in ad, preprocess it, remove duplicates, add total interactions and URL, and append to the cleaned_dataframes list\n",
    "for df in ad:\n",
    "    if not df.empty:\n",
    "        cleaned_df = preprocess_data(df)\n",
    "        cleaned_df = remove_duplicates(cleaned_df)\n",
    "        cleaned_df = add_total_interactions(cleaned_df)\n",
    "        cleaned_df = add_video_url(cleaned_df)\n",
    "        cleaned_dataframes.append(cleaned_df)\n",
    "    else:\n",
    "        cleaned_dataframes.append(df)  # Append the empty dataframe if it was not loaded\n",
    "\n",
    "# Save cleaned DataFrames to CSV files in the specified directory\n",
    "for cleaned_df, category in zip(cleaned_dataframes, categories):\n",
    "    # Remove invalid characters from category names for filenames\n",
    "    valid_category = category.replace(' & ', '_and_').replace(' ', '_')\n",
    "    output_file = os.path.join(save_directory, f'dash_csv_{valid_category}.csv')\n",
    "    cleaned_df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved preprocessed data to {output_file}\")\n",
    "\n",
    "# Display the first few rows of one of the cleaned dataframes with total interactions and URLs\n",
    "if not cleaned_dataframes[3].empty:\n",
    "    print(cleaned_dataframes[3].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65b9a4a9-e4b8-4355-8fd8-9475b84eb485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad09f88a-bf6e-4d66-a49b-5d2cd9860615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
